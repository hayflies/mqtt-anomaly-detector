import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, recall_score, f1_score
from xgboost import XGBClassifier
import joblib
import os

# ğŸ“¥ 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("data/processed/cleaned_binary.csv")

# ğŸ¯ 2. íŠ¹ì„±ê³¼ ë¼ë²¨ ì •ì˜
features = [
    'tcp.flags', 'tcp.time_delta',
    'mqtt.kalive', 'mqtt.qos', 'mqtt.retain', 'mqtt.len',
    'mqtt.dupflag', 'mqtt.msgtype'
]
X = df[features]
y = df['binary_label']

# ğŸ§ª 3. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# ğŸ”„ 4. ì •ê·œí™”
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# âš–ï¸ 5. scale_pos_weight ê³„ì‚° (ê³µê²© ë¹„ìœ¨ ì ì„ìˆ˜ë¡ ë” ë†’ê²Œ ì„¤ì •ë¨)
normal_count = sum(y_train == 0)
attack_count = sum(y_train == 1)
scale_pos_weight = (normal_count / attack_count) * 0.5

# ğŸš€ 6. ëª¨ë¸ í•™ìŠµ
model = XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    scale_pos_weight=scale_pos_weight,
    reg_alpha=0.5,  # L1 ê·œì œ
    reg_lambda=1.0, # L2 ê·œì œ
    eval_metric='logloss',
    random_state=42
)
model.fit(X_train_scaled, y_train)

# ğŸ“Š 7. í‰ê°€
y_pred = model.predict(X_test_scaled)

recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("ğŸ“Š ì´ì§„ ë¶„ë¥˜ ì„±ëŠ¥ ë¦¬í¬íŠ¸:\n", classification_report(y_test, y_pred))
print(f"ğŸ¯ ê³µê²©(1) í´ë˜ìŠ¤ ê¸°ì¤€ Recall: {recall:.4f}")
print(f"ğŸ¯ ê³µê²©(1) í´ë˜ìŠ¤ ê¸°ì¤€ F1-Score: {f1:.4f}")

# ğŸ’¾ 8. ëª¨ë¸ ì €ì¥
os.makedirs("models", exist_ok=True)
joblib.dump(model, "models/xgb_binary_model.pkl")
joblib.dump(scaler, "models/xgb_binary_scaler.pkl")
print("âœ… ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ (models/xgb_binary_model.pkl)")
